{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopandas import GeoDataFrame, GeoSeries\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will find the project directory so that data can be read in\n",
    "directory = Path.cwd()\n",
    "# Look up through parents until we find the base ladi-tutorial folder\n",
    "matching_parent = [p for p in directory.parents if p.name.lower()=='ladi-tutorial']\n",
    "if matching_parent:\n",
    "    # if we find it, then it's the first entry\n",
    "    tutorial_dir = matching_parent[0]\n",
    "else:\n",
    "    # otherwise, raise an error\n",
    "    raise OSError(f'Notebook needs to be run from within the `ladi-tutorial` directory. Current directory is {str(directory)}')\n",
    "print(tutorial_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following datasets will be used in this analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://catalog.data.gov/dataset/tiger-line-shapefile-2017-nation-u-s-current-state-and-equivalent-national\n",
    "file = tutorial_dir / 'data/Census-State/tl_2017_us_state.shp'\n",
    "states = gpd.read_file(file)\n",
    "# This line sets the CRS (Coordinate Reference System) so that all our maps will line up when plotting\n",
    "states = states.to_crs(epsg=4326) \n",
    "\n",
    "# https://ais-faa.opendata.arcgis.com/datasets/e747ab91a11045e8b3f8a3efd093d3b5_0\n",
    "file = tutorial_dir / 'data/FAA-Airports/Airports.shp'\n",
    "airports = gpd.read_file(file)\n",
    "airports = airports.to_crs(epsg=4326)\n",
    "\n",
    "# https://catalog.data.gov/dataset/tiger-line-shapefile-2019-nation-u-s-current-metropolitan-statistical-area-micropolitan-statist\n",
    "file = tutorial_dir / 'data/Census-CBSA/tl_2019_us_cbsa.shp'\n",
    "us_cbsa = gpd.read_file(file)\n",
    "us_cbsa = us_cbsa.to_crs(epsg=4326)\n",
    "\n",
    "# http://www2.census.gov/programs-surveys/ahs/2017/AHS%202017%20National%20PUF%20v3.0%20Flat%20CSV.zip?#\n",
    "file = tutorial_dir / 'data/Census-AHS/ahs2017n.csv'\n",
    "ahs_data = pd.read_csv(file, usecols=['OMB13CBSA', 'DPFLDINS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meters_to_degrees(distance_meters):\n",
    "    \n",
    "    '''https://sciencing.com/convert-distances-degrees-meters-7858322.html (111,139)'''\n",
    "    \n",
    "    distance_degrees = (distance_meters / 111194.926644559) # number derived from matlab calculations\n",
    "    return distance_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_gdf(df, what_on): \n",
    "    \n",
    "    '''df_to_gdf merges a given DataFrame with image_metadata_gdf on whatever column name they choose'''\n",
    "    \n",
    "    merged_df = df.merge(image_metadata_gdf, on=what_on)\n",
    "    clean_df = merged_df.dropna()\n",
    "    gdf = GeoDataFrame(clean_df, crs='epsg:4326')\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_finder(df):  \n",
    "    \n",
    "    '''state_finder performs a spacial join with the states shapefile to find what state points are plotted in'''\n",
    "    \n",
    "    label_by_state = gpd.sjoin(df, states, op='within')\n",
    "    label_by_state = label_by_state.drop(columns=['index_right'])\n",
    "    \n",
    "    return(label_by_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_values(dataset_name, dataset_column): \n",
    "    \n",
    "    '''get_column_values returns a dictionary that tells us how many of each value was found for a given column in a dataset'''\n",
    "    \n",
    "    column_dictionary_counter = {} \n",
    "    values = dataset_name[dataset_column].tolist()\n",
    "    for i in values:\n",
    "        if i not in column_dictionary_counter:\n",
    "            column_dictionary_counter[i] = values.count(i)\n",
    "            \n",
    "    return(column_dictionary_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def months_with_images(state_abbreviation, label, year): \n",
    "    \n",
    "    '''This function returns a dictionary telling the user how many photos were taken each month for the given dataset'''\n",
    "    \n",
    "    # Merge the images with the states to get a state name for each image\n",
    "    images_by_state = state_finder(label)\n",
    "\n",
    "    # Find images that were taken during the given timestamp\n",
    "    state_images = images_by_state[images_by_state.STUSPS == state_abbreviation]\n",
    "    state_images_timestamp = state_images[state_images['timestamp'].str.contains(year, na=False, case=True)]\n",
    "    \n",
    "    # Create a dictionary with the month names as keys and how many images were taken as their values\n",
    "    months = {}\n",
    "    for i in range(len(state_images_timestamp.timestamp)):\n",
    "        month = pd.Timestamp(state_images_timestamp.timestamp.iloc[i])\n",
    "        if month.month_name() not in months:\n",
    "            state_images_per_month = 0\n",
    "            \n",
    "            for j in range(len(state_images_timestamp.timestamp)):\n",
    "                month_counter = pd.Timestamp(state_images_timestamp.timestamp.iloc[j])\n",
    "                if month.month_name() == month_counter.month_name():\n",
    "                    state_images_per_month += 1\n",
    "                months[month.month_name()] = state_images_per_month\n",
    "    return(print(months))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def months_with_images(state_abbreviation, label, year): \n",
    "    \n",
    "    '''This function returns a dictionary telling the user how many photos were taken each month for the given dataset'''\n",
    "    \n",
    "    # Merge the images with the states to get a state name for each image\n",
    "    images_by_state = state_finder(label)\n",
    "\n",
    "    # Find images that were taken during the given timestamp\n",
    "    state_images = images_by_state[images_by_state.STUSPS == state_abbreviation]\n",
    "    state_images_timestamp = state_images[state_images['timestamp'].str.contains(year, na=False, case=True)]\n",
    "    \n",
    "    # Create a dictionary with the month names as keys and how many images were taken as their values\n",
    "    months = {}\n",
    "    for i in range(len(state_images_timestamp.timestamp)):\n",
    "        month = pd.Timestamp(state_images_timestamp.timestamp.iloc[i])\n",
    "        if month.month_name() not in months:\n",
    "            state_images_per_month = 0\n",
    "            \n",
    "            for j in range(len(state_images_timestamp.timestamp)):\n",
    "                month_counter = pd.Timestamp(state_images_timestamp.timestamp.iloc[j])\n",
    "                if month.month_name() == month_counter.month_name():\n",
    "                    state_images_per_month += 1\n",
    "                months[month.month_name()] = state_images_per_month\n",
    "    return(print(months))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_per_cbsa(gdf, cbsa): \n",
    "    \n",
    "    '''images_per_cbsa tells us how many points from your GeoDataFrame were found within each cbsa code'''\n",
    "    \n",
    "    image_counter = {}\n",
    "    \n",
    "    # Perform a spacial join between cbsa codes and the given GeoDataFrame\n",
    "    images_within_cbsa = gpd.sjoin(gdf, cbsa, how='left', op='within')\n",
    "    images = images_within_cbsa.NAMELSAD.tolist()\n",
    "\n",
    "    # Create a dictionary with the CBSA code names as keys and how many images were taken as their values\n",
    "    for i in images:\n",
    "        if i not in image_counter:\n",
    "            image_counter[i] = images.count(i)\n",
    "    \n",
    "    # make the dictionary into a dataframe \n",
    "    image_counter = pd.DataFrame.from_dict(image_counter, orient='index')\n",
    "    \n",
    "    \n",
    "    return image_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we read in the image metadata from the LADI s3 bucket. We will use this data along with our human and machine labels to make a geospatial analysis of the destruction from Hurricanes Michael and Florence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'ladi'\n",
    "file_1_path = 'Labels/ladi_images_metadata.csv'\n",
    "client = boto3.client('s3')\n",
    "\n",
    "obj_1 = client.get_object(Bucket = bucket_name, Key = file_1_path)\n",
    "\n",
    "image_metadata = pd.read_csv(obj_1['Body'])\n",
    "image_metadata_renamed = image_metadata.rename(columns={\"uuid\": \"image_uuid\"})\n",
    "image_metadata_clean = image_metadata_renamed.dropna()\n",
    "\n",
    "latitude = image_metadata_clean['gps_lat'].tolist() \n",
    "longitude = image_metadata_clean['gps_lon'].tolist()\n",
    "\n",
    "#This line converts the DF to a GDF and sets the proper crs\n",
    "image_metadata_gdf = GeoDataFrame(image_metadata_clean, crs='epsg:4326', geometry=gpd.points_from_xy(longitude, latitude))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_metadata_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we read in the LADI human labels dataset, specifically those with the label 'damage' or 'flood' because we want to see how many of the images taken actually contain damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### CLEAN AND VALIDATE LADI HUMAN LABELS #########################\n",
    "human_label_filepath = \"Labels/ladi_aggregated_responses_url.tsv\"\n",
    "obj_2 = client.get_object(Bucket = bucket_name, Key = human_label_filepath)\n",
    "human_label_file = pd.read_csv(obj_2['Body'],sep = '\\t' )\n",
    "\n",
    "#STRIP OFF BRACKET AND COMMA FROM THE ANSWER CATEGORY\n",
    "human_label_file[\"Answer\"] = human_label_file[\"Answer\"].str.strip('[|]')\n",
    "human_label_file[\"Answer\"] = human_label_file[\"Answer\"].str.split(\",\",expand = True)\n",
    "\n",
    "#EXTRACT LABELS WITH DAMAGE AND INFRASTRUCTURE CATEGORIES AND REMOVE THOSE LABELED 'NONE'\n",
    "label_damage_infra = human_label_file[human_label_file['Answer'].str.contains('damage|infrastructure',na=False,case=False)]\n",
    "label_clean = label_damage_infra[~label_damage_infra['Answer'].str.contains('none',na=False,case=False)]\n",
    "human_flood_label = label_clean[label_clean['Answer'].str.contains('flood',na=False,case=False)]\n",
    "human_damage_label = label_clean[label_clean['Answer'].str.contains('damage',na=False,case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that the human_damage_label dataset doesn't have alot of content so we will merge it with image_metadata_gdf to get all of the data we want for these specific images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_damage_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_labeled_damage = df_to_gdf(human_damage_label, 'url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_labeled_damage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_flood_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_labeled_floods = df_to_gdf(human_flood_label, 'url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_labeled_floods.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the code to read in the Ladi machine labels (~10 mins @ 100Mbps):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################### CLEAN AND VALIDATE LADI MACHINE LABELS #########################\n",
    "\n",
    "machine_label_filepath = \"Labels/ladi_machine_labels.csv\"\n",
    "obj_3 = client.get_object(Bucket = bucket_name, Key = machine_label_filepath)\n",
    "machine_flood_label = pd.read_csv(obj_3['Body'], usecols=['image_uuid', 'label_text'])\n",
    "machine_flood_label_clean = machine_flood_label[machine_flood_label['label_text'].str.contains('flood', na=False,case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "machine_labeled_floods = df_to_gdf(machine_flood_label_clean, 'image_uuid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(len(machine_labeled_floods))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lst = []\n",
    "for i in machine_labeled_floods.timestamp:\n",
    "    if timestamp.str.contains('2019'):\n",
    "        lst.append(i)\n",
    "print(len(lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will create a buffer (circle) around each of the airports at a given radius from the center changing the geometric Points to Polygons. We will use these buffer radius' to see what images are within a 5 mile range of one of the airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.geometry = airports.geometry.buffer(meters_to_degrees(8046.72)) #equal to 5 miles in meters\n",
    "airports.geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can filter out images based on state, the year they were taken and the label dataset (if the condition is set to true it will plot airports and images that were taken within the buffer radius of the airport (default: 5 miles)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_plotter(state_abbreviation, df, year, conditional): \n",
    "    \n",
    "    '''State plotter returns a map of the desired state and plots the points of flooding or disaster imagery \n",
    "       stored in the DataFrame and based on the given timeframe e.g. (year: '2018', month: '2018-10'). If \n",
    "       the conditional is True only images within the buffer radius of an airport will be plotted. Otherwise\n",
    "       we will ignore all of the airports and plot all of the disaster images within the desired state'''\n",
    "    \n",
    "    if conditional is True:# Plot the Airports and only images within their buffer radius\n",
    "        \n",
    "        # ax is the matplotlib axis object. Setting this around the desired state will set the map boundary for the rest to follow \n",
    "        ax = states[states.STUSPS == state_abbreviation].plot(figsize=(10,10), alpha = .3, edgecolor = 'k')\n",
    "        \n",
    "        #Find the airports within the desired state\n",
    "        airports_by_state = state_finder(airports) \n",
    "        state_airports = airports_by_state[airports_by_state.STUSPS == state_abbreviation]\n",
    "        \n",
    "        # Find the images given the state and the timestamp \n",
    "        images_by_state = state_finder(df)\n",
    "        state_images = images_by_state[images_by_state.STUSPS == state_abbreviation]\n",
    "        state_images_timestamp = state_images[state_images['timestamp'].str.contains(year, na=False, case=True)]\n",
    "        \n",
    "        # Plot the images that are within the buffer radius of an airport\n",
    "        images_within_range = gpd.sjoin(state_images_timestamp, state_airports, op='within')\n",
    "        images_within_range = images_within_range.drop(columns=['index_right'])\n",
    "        images_within_range.plot(ax=ax, marker='.', markersize = 5, color='red', zorder=3)\n",
    "        \n",
    "        # Plot the Airports that have at least 1 image within range\n",
    "        airports_within_range = gpd.sjoin(state_airports, state_images_timestamp, op='contains')\n",
    "        airports_within_range.plot(ax=ax, color='black', alpha=.5, zorder=2)\n",
    "        \n",
    "        # Find the CBSA codes for the given State and plot them (STUSPS is the calumn name for state abbreviations)\n",
    "        cbsa_by_state = state_finder(us_cbsa)\n",
    "        state_cbsa = cbsa_by_state[cbsa_by_state['STUSPS'].str.contains(state_abbreviation, na=False, case=True)]\n",
    "        state_cbsa.plot(ax=ax, alpha= .5, edgecolor = 'black', zorder=1)\n",
    "      \n",
    "        # Contextily has some nice basemaps this is how you would add one \n",
    "        ctx.add_basemap(ax, crs = state_cbsa.crs)\n",
    "            \n",
    "        # Find the CBSA code names and the number of images found within each as a DataFrame\n",
    "        num_images_per_cbsa = images_per_cbsa(images_within_range, state_cbsa)\n",
    "        count = len(images_within_range)\n",
    "        \n",
    "        return(plt.show(), print('Total images: ', count), print(num_images_per_cbsa)) # plt.show() overlays the maps\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # ax is the matplotlib axis object. Setting this around the state will set the map boundary for the rest to follow \n",
    "        ax = states[states.STUSPS == state_abbreviation].plot(figsize=(10,10), alpha = .3, edgecolor = 'k')\n",
    "       \n",
    "        # Find images in the given state taken during the given timeframe and plot these images\n",
    "        images_by_state = state_finder(df)\n",
    "        state_images = images_by_state[images_by_state.STUSPS == state_abbreviation]\n",
    "        state_images_timestamp = state_images[state_images['timestamp'].str.contains(year, na=False, case=True)]\n",
    "        state_images_timestamp.plot(ax=ax, marker='.', markersize = 5, color='red', zorder=3)\n",
    "        \n",
    "        # Find the CBSA codes for the given State and plot them (STUSPS is the calumn name for state abbreviations)\n",
    "        cbsa_by_state = state_finder(us_cbsa)\n",
    "        state_cbsa = cbsa_by_state[cbsa_by_state['STUSPS'].str.contains(state_abbreviation, na=False, case=True)]\n",
    "        state_cbsa.plot(ax=ax, alpha= .5, edgecolor = 'black', zorder=1)\n",
    "        \n",
    "        # Contextily has some nice basemaps this is how you would add one \n",
    "        ctx.add_basemap(ax, crs = state_cbsa.crs)\n",
    "        \n",
    "        # Find the CBSA code names and the number of images found within each as a DataFrame\n",
    "        num_images_per_cbsa = images_per_cbsa(state_images_timestamp, state_cbsa)\n",
    "        count = len(state_images_timestamp)\n",
    "\n",
    "        return(plt.show(), print('Total images: ', count), print(num_images_per_cbsa)) # plt.show() overlays the maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hurricane Michael:\n",
    "- MAKES LANDFALL IN FLORIDA ON OCTOBER 10TH 2018\n",
    "- MICHAEL COMES UP THROUGH THE GULF COAST PUSHES NORTH THROUGH TALLAHASSE AND CONTINUES INTO GEORGIA\n",
    "- WE CAN SEE THAT A MAJORITY OF THE IMAGES WERE TAKEN IN OCTOBER (THE SAME MONTH IT HIT) \n",
    "- ONLY ONE FOLLOW UP IMAGE WAS TAKEN IN NOVEMBER, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n2018: ')\n",
    "state_plotter('FL', image_metadata_gdf, '2018', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_with_images('FL', image_metadata_gdf, '2018') #returns the number of images taken in FL in 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n2018: ')\n",
    "state_plotter('FL', image_metadata_gdf, '2018', False)\n",
    "print('\\nOCTOBER 2018: ')\n",
    "state_plotter('FL', image_metadata_gdf, '2018-10', False)\n",
    "#black circles represent the given buffer radius for the airports\n",
    "#red dots are points for disaster images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_with_images('FL', human_labeled_floods, '2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n2018: ')\n",
    "state_plotter('FL', human_labeled_floods, '2018', False)\n",
    "print('\\nOCTOBER 2018: ')\n",
    "state_plotter('FL', human_labeled_floods, '2018-10', False) # returns only images taken in October 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_with_images('GA', image_metadata_gdf, '2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n2018: ')\n",
    "state_plotter('GA', image_metadata_gdf, '2018', False)\n",
    "print('\\nOCTOBER 2018: ')\n",
    "state_plotter('GA', image_metadata_gdf, '2018-10', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_with_images('GA', human_labeled_floods, '2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2018: ')\n",
    "state_plotter('GA', human_labeled_floods, '2018', False)\n",
    "print('\\nOCTOBER 2018: ')\n",
    "state_plotter('GA', human_labeled_floods, '2018-10', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hurricane Florence:\n",
    "- HIT CAROLINAS DIRECTLY ALONG COAST LINE (AUGUST 31, 2018-SEPTEMBER 18TH, 2018)\n",
    "- MAKES LANDFALL IN NC ON SEPTEMBER 14TH\n",
    "- PUSHES WESTWARD THROUGH THE CAROLINAS AND INTO GEORGIA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_with_images('NC', image_metadata_gdf, '2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2018: ')\n",
    "state_plotter('NC', image_metadata_gdf, '2018', False)\n",
    "print('\\nSEPTEMBER 2018: ')\n",
    "state_plotter('NC', image_metadata_gdf, '2018-09', False)\n",
    "print('\\nOCTOBER 2018: ')\n",
    "state_plotter('NC', image_metadata_gdf, '2018-10', False)\n",
    "print('\\nNOVEMBER 2018: ')\n",
    "state_plotter('NC', image_metadata_gdf, '2018-11', False)\n",
    "print('\\nDECEMBER 2018: ')\n",
    "state_plotter('NC', image_metadata_gdf, '2018-12', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_with_images('NC', human_labeled_floods, '2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2018: ')\n",
    "state_plotter('NC', human_labeled_floods, '2018', False)\n",
    "print('\\nSEPTEMBER 2018: ')\n",
    "state_plotter('NC', human_labeled_floods, '2018-09', False)\n",
    "print('\\nOCTOBER 2018: ')\n",
    "state_plotter('NC', human_labeled_floods, '2018-10', False)\n",
    "print('\\nNOVEMBER 2018: ')\n",
    "state_plotter('NC', human_labeled_floods, '2018-11', False)\n",
    "print('\\nDECEMBER 2018: ')\n",
    "state_plotter('NC', human_labeled_floods, '2018-12', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_with_images('SC', image_metadata_gdf, '2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2018: ')\n",
    "state_plotter('SC', image_metadata_gdf, '2018', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_with_images('SC', human_labeled_floods, '2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2018: ')\n",
    "state_plotter('SC', human_labeled_floods, '2018', False)\n",
    "print('\\nSEPTEMBER 2018: ')\n",
    "state_plotter('SC', human_labeled_floods, '2018-09', False)\n",
    "print('\\nOCTOBER 2018: ')\n",
    "state_plotter('SC', human_labeled_floods, '2018-10', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_with_images('FL', image_metadata_gdf, '2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2018: ')\n",
    "state_plotter('FL', image_metadata_gdf, '2018', False)\n",
    "print('\\nAUGUST 2018: ')\n",
    "state_plotter('FL', image_metadata_gdf, '2018-08', False)\n",
    "print('\\nSEPTEMBER 2018: ')\n",
    "state_plotter('FL', image_metadata_gdf, '2018-09', False)\n",
    "print('\\nOCTOBER 2018: ')\n",
    "state_plotter('FL', image_metadata_gdf, '2018-10', False)\n",
    "print('\\nNOVEMBER 2018: ')\n",
    "state_plotter('FL', image_metadata_gdf, '2018-11', False)\n",
    "print('\\nDECEMBER 2018: ')\n",
    "state_plotter('FL', image_metadata_gdf, '2018-12', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_with_images('FL', human_labeled_floods, '2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2018: ')\n",
    "state_plotter('FL', human_labeled_floods, '2018', False)\n",
    "print('\\nOCTOBER 2018: ')\n",
    "state_plotter('FL', human_labeled_floods, '2018-10', False)\n",
    "print('\\nNOVEMBER 2018: ')\n",
    "state_plotter('FL', human_labeled_floods, '2018-11', False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
