{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopandas import GeoDataFrame, GeoSeries\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon\n",
    "from pathlib import Path\n",
    "from pyproj import CRS\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "import fiona\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will find the project directory so that data can be read in\n",
    "directory = Path.cwd()\n",
    "# Look up through parents until we find the base ladi-tutorial folder\n",
    "matching_parent = [p for p in directory.parents if p.name.lower()=='ladi-tutorial']\n",
    "if matching_parent:\n",
    "    # if we find it, then it's the first entry\n",
    "    tutorial_dir = matching_parent[0]\n",
    "else:\n",
    "    # otherwise, raise an error\n",
    "    raise OSError(f'Notebook needs to be run from within the `ladi-tutorial` directory. Current directory is {str(directory)}')\n",
    "print(tutorial_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we read in the Natural Earth data so that we can plot our images and states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-1-states-provinces/\n",
    "file = tutorial_dir / 'data/Natural-Earth/ne_10m_admin_1_states_provinces.shp'\n",
    "natural_earth = gpd.read_file(file)\n",
    "natural_earth = natural_earth.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = natural_earth.plot(figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we read in the image metadata and human aggregated response data from LADI's AWS bucket (specifically those with a 'flood' or 'damage' label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'ladi'\n",
    "file_1_path = 'Labels/ladi_images_metadata.csv'\n",
    "client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_1 = client.get_object(Bucket = bucket_name, Key = file_1_path)\n",
    "\n",
    "image_metadata = pd.read_csv(obj_1['Body'])\n",
    "image_metadata_renamed = image_metadata.rename(columns={\"uuid\": \"image_uuid\"})\n",
    "image_metadata_clean = image_metadata_renamed.dropna()\n",
    "\n",
    "latitude = image_metadata_clean['gps_lat'].tolist() \n",
    "longitude = image_metadata_clean['gps_lon'].tolist()\n",
    "\n",
    "#This line converts the DF to a GDF and sets the proper crs\n",
    "image_metadata_gdf = GeoDataFrame(image_metadata_clean, crs='epsg:4326', geometry=gpd.points_from_xy(longitude, latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### CLEAN AND VALIDATE LADI HUMAN LABELS #########################\n",
    "human_label_filepath = \"Labels/ladi_aggregated_responses_url.tsv\"\n",
    "obj_2 = client.get_object(Bucket = bucket_name, Key = human_label_filepath)\n",
    "human_label_file = pd.read_csv(obj_2['Body'],sep = '\\t' )\n",
    "\n",
    "#STRIP OFF BRACKET AND COMMA FROM THE ANSWER CATEGORY\n",
    "human_label_file[\"Answer\"] = human_label_file[\"Answer\"].str.strip('[|]')\n",
    "human_label_file[\"Answer\"] = human_label_file[\"Answer\"].str.split(\",\",expand = True)\n",
    "\n",
    "#EXTRACT LABELS WITH DAMAGE AND INFRASTRUCTURE CATEGORIES AND REMOVE THOSE LABELED 'NONE'\n",
    "label_damage_infra = human_label_file[human_label_file['Answer'].str.contains('damage|infrastructure',na=False,case=False)]\n",
    "label_clean = label_damage_infra[~label_damage_infra['Answer'].str.contains('none',na=False,case=False)]\n",
    "human_flood_label = label_clean[label_clean['Answer'].str.contains('flood',na=False,case=False)]\n",
    "human_damage_label = label_clean[label_clean['Answer'].str.contains('damage',na=False,case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merger(df, gdf, what_on): \n",
    "    \n",
    "    '''merger merges a given DataFrame with the given GeoDataFrame such as image_metadata_gdf on \n",
    "       whatever column they choose, so that we can plot these dataframes'''\n",
    "    \n",
    "    merged_df = gdf.merge(df, on=what_on)\n",
    "    gdf = GeoDataFrame(merged_df, crs='epsg:4326')\n",
    "    \n",
    "    return(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_finder(df):  \n",
    "    \n",
    "    '''state_finder performs a spacial join with the states\n",
    "       shapefile to find which state points are plotted in'''\n",
    "    \n",
    "    label_by_state = gpd.sjoin(df, states, op='within')\n",
    "    label_by_state = label_by_state.drop(columns=['index_right'])\n",
    "    \n",
    "    return(label_by_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_per_state(timestamp):\n",
    "\n",
    "    '''images_per_states counts how many images were taken in each state for \n",
    "       the given timeframe above (states not printed if they have no images):'''\n",
    "\n",
    "    state_counter = {}\n",
    "    state_names = []\n",
    "\n",
    "    for i in timestamp.name_en:\n",
    "        state_names.append(i)\n",
    "\n",
    "    for i in state_names:\n",
    "        if i not in state_counter:\n",
    "            state_counter[i] = state_names.count(i)\n",
    "\n",
    "    return(state_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories(state_counter):\n",
    "    \n",
    "    '''state_counter takes the values above, finds the highest value\n",
    "       and divides that by three to create numerical categories for states'''\n",
    "\n",
    "    max_value=0\n",
    "    for i in state_counter.values():\n",
    "        if i > max_value:\n",
    "            max_value = i\n",
    "\n",
    "    low = int(max_value / 3)\n",
    "    mid = int(low * 2)\n",
    "    high = int(max_value)\n",
    "\n",
    "    return(low, mid, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_category_splitter(low, mid, high, state_counter):\n",
    "    \n",
    "    '''state_category_splitter places each state into an array based on the number of \n",
    "       images within that state and the predetermined low, mid, and high values'''\n",
    "    \n",
    "    low_states = []\n",
    "    mid_states = []\n",
    "    high_states = []\n",
    "\n",
    "    for i in state_counter.keys():\n",
    "        val = state_counter.get(i)\n",
    "        if val <= low:\n",
    "            low_states.append(i)\n",
    "        if val > low and val < high:\n",
    "            mid_states.append(i)\n",
    "        if val >= high:\n",
    "            high_states.append(i)\n",
    "\n",
    "    return(low_states, mid_states, high_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_dictionary_counter(low, mid, high, state_counter):\n",
    "\n",
    "    low_states_count = {}\n",
    "    mid_states_count = {}\n",
    "    high_states_count = {}\n",
    "\n",
    "    for i in state_counter.keys():\n",
    "        val = state_counter.get(i)\n",
    "        if val <= low:\n",
    "            low_states_count[i] = val\n",
    "        if val > low and val < high:\n",
    "            mid_states_count[i] = val\n",
    "        if val >= high:\n",
    "            high_states_count[i] = val\n",
    "\n",
    "    return(print('Low: ', low_states_count, '\\n\\n\\nMid: ', mid_states_count, '\\n\\n\\nHigh: ', high_states_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_plotter(gdf, timeframe):\n",
    "    \n",
    "    '''state_plotter The followi function will calculate the number of images in each US State,\n",
    "       given a GeoDataFrame and a timeframe (e.g. year: '2018', month: '2018-10').\n",
    "       It will then generate a map with each USA state color coded based on the number of images.'''  \n",
    "    \n",
    "    # Merge any DataFrame with states to find what state images were taken in\n",
    "    state_gdf = state_finder(gdf)\n",
    "    \n",
    "    # Determines what images in the GeoDataFrame were taken during the given timeframe e.g.(timeframe: '2018', month: '2018-10')\n",
    "    state_timestamp_gdf = state_gdf[state_gdf['timestamp'].str.contains(timeframe, na=False, case=True)]\n",
    "    \n",
    "    # Count the number of images in each state, find the highest value, and divide that by three to categorize each state\n",
    "    count = images_per_state(state_timestamp_gdf)\n",
    "    temp = categories(count)\n",
    "\n",
    "    # split the states into different arrays based on the number of images within that state\n",
    "    arrays = state_category_splitter(temp[0], temp[1], temp[2], count)\n",
    "    state_dictionary_counter(temp[0], temp[1], temp[2], count)\n",
    "    low_states = arrays[0]\n",
    "    mid_states = arrays[1]\n",
    "    high_states = arrays[2]\n",
    "\n",
    "    # Make each of the arrays into DataFrames so they can be merged with states\n",
    "    low_states_df = pd.DataFrame(low_states, columns=['name'])\n",
    "    mid_states_df = pd.DataFrame(mid_states, columns=['name'])\n",
    "    high_states_df = pd.DataFrame(high_states, columns=['name'])\n",
    "\n",
    "    # Now we will make a GDF from each of the arrays so that each state can be plotted based on their number of images:\n",
    "    low_states_gdf = merger(low_states_df, states, 'name')\n",
    "    mid_states_gdf = merger(mid_states_df, states, 'name')\n",
    "    high_states_gdf = merger(high_states_df, states, 'name')\n",
    "\n",
    "    # Now that we have the states split up into their seperate GeoDataFrames we can plot each their own color:\n",
    "    ax = low_states_gdf.plot(figsize=(20,20), color='green', alpha=.7, edgecolor='black')\n",
    "    mid_states_gdf.plot(ax=ax, figsize=(20,20), color='yellow', alpha=.7, edgecolor='black')\n",
    "    high_states_gdf.plot(ax=ax, figsize=(20,20), color='red', alpha=.7, edgecolor='black')\n",
    "    ctx.add_basemap(ax, crs = 'epsg:4326')\n",
    "    \n",
    "    return (plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = natural_earth[natural_earth.iso_a2 == 'US'] #filters map down to just US States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image_metadata_gdf map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_plotter(image_metadata_gdf, '2019')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## human_labeled_floods map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_labeled_floods = merger(human_flood_label, image_metadata_gdf, 'url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_plotter(human_labeled_floods, '2019')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## human_labeled_damage map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_labeled_damage = merger(human_damage_label, image_metadata_gdf, 'url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_plotter(human_labeled_damage, '2019')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
